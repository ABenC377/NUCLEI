To seperate my concerns, I've put all of the lexer code in a seperate .c and .h file, and have tested this seperately.  Then my parser and interpreter are in the nuclei.c/.h files.  I've explained the testing strategy for both, seperately, below.

___Lexer testing___

The following functions are assert tested in the lexical_parse_test() function as described:
- get_file_name()
    This function takes argc and argv as inputs, and outputs the file name if one is provided as an argv argument.  To check this, it looks for an argument that has a '.' in a non-final position.  If there are more than one file names provided, then the function returns the first file name provided.  If there are no file names provided, then the function returns NULL.  To test this function, I have assert tested that it provides the frist correct file name when given sets of argv strings that contain a valid file name, and NULL when it there isn't a correct file name.
    The argc and argv arguments for this function are passed directly from the command line, and so argc and argv should be consistent (i.e., there should be argc elements in argv).  Therefore, the function does not handle inconsistent arguments, and I have not tested its behavious with inconsistent arguments.

- update_tokens()
    This function takes in the list of tokens, automata, and a character as arguments.  The behaviour is dictated by the state of the automata - it is a large switch statement.
    For the automata states start, in_literal and in_string, this function simply calls another handling function.  Therefore, to avoid duplication, the testing of this function for these cases is done in conjunction with the testing of the respective handling functions.
    for the other automata states, the function checks whether the input character is the anticipated next character in a/the reserved word or not.  If it is, then it moves on to the next state.  Otherwise, it calls a function (add_previous_chars()) that retroactively handles the letters of the state knowing that it won't end as a reserved word.  I have tested these states by running through the anticipated sequence of characters to get to each of the reserved words and assert tested that these work.
    For cases where an unexpected characters are provided, the testing is done in conjunction with the add_previous_chars function, to avoid duplication.

- add_previous_chars()
    This function is for backtracking when you have already recieved a number of letters of a reserved word (e.g., WHILE) and then recieved an unexpected character.  It does this by adding the first letter of the reserved word as a variable token to the token list, and then considering the following letters in order as if they were newly input characters.  To test this function, I checked that the function correctly adds the variable to a token list, and then seperately that it correctly adds the following characters (either as variables or as different reserved words).

- make_and_add_simple_token()
    This function makes a token of a given type, adds it to the tokens list, and resets the automata to its start state.  It only works for token types that don't have additional information (i.e., all but literal, string and variable token types).  I have tested this function against each of the simple token types to check that it successfully adds a token of that type to the token list.  This function is only ever called when a simple token is to be added, and so there is no need to test it with complex token type inputs.

- handle_start_state()
    This function checks the character that is provided from the file, and handles it correctly.  There are five types of valid characters according to the grammar of NUCLEI: 1) white space - this should be ignored; 2) parentheses - these are individual tokens and should be added to the token list; 3) a single quote - this signifies the beginning of a literal; 4) a double quote - this signifies the beginning of a string; and 5) a capitalised letter - this is either a variable or the beginning of a reserved word, and should be handled accordingly.  If the character is none of these, then the function adds an 'invalid' token to the token list.  To test this function, I have assert tested it with characters of each of the above types (including invalid) to ensure that they are handled correctly.
    This function is only called from update_tokens(), when the automata is in the start state.  It is passed the tokens and automata arguments directly from the arguments of this higher function.  Therefore, we do not need to check that these arguments are valid, because this has been checked higher up.  Therefore, I have not tested for this.

- add_variable_token()
    This function adds a token of type variable to the token list.  The name of the variable (i.e., A-Z) is provided as an input.  I've tested this function against each possible varibale name to confirm that a correctly named token is added to the token list.  
    This function is only called in add_previous_chars() and handle_start_state().  in add_previous_chars(), it is called in relation to the var argument, which is only ever a capitalised letter (see the switch statement in update_tokens()).  In handle_start_state(), this function is only called in the event of a character that is A-Z.  Therefore, we know that this function is only ever called with a name that is valid (i.e., A-Z).  Therefore, we do not need to handle incorrect names, and I have not tested for this.
    Similarly, because the tokens and automata arguments of this function are always passed directly from the arguments of the functions that call it (i.e., add_previous_chars() and handle_start_state()), we know that they must be valid (otherwise the higher up functions would have failed).  Therefore, I have not tested for this.

- handle_in_state()
    This function is called when a character is recieved while the automata is in either the 'in_literal' or'in_string' state.  It checks whether the character is ending the literal/string (i.e., whether it it a single or double quote, respectively).  If it is ending the literal/string, then it adds the token to the token list.  Otherwise it adds the new character to the lexeme string of the token associated with the automata.  I have tested this function by running it in both 'in_literal' and 'in_string' states with both closing and non-closing characters, to make sure that the string is updated correctly, and that the token is added to the token list correctly.
    Because this function is only called from update_tokens() when the automata state is either 'in_string' or 'in_literal' (see the switch statement in update_tokens()), I have not tested the function with non-intenedd automata states.  Further, the tokens and automata arguments are carried over directly from the update_tokens arguments, and so have already been checked.  Therefore, unintended values are not tested for this function.

- add_token()
    This function adds a token to the end of a token list.  I have tested it with tokens of every token type to ensure that it correctly adds the token to the token list, as well as providing it with a NULL token pointer to ensure that it simply leaves the token list as it.
    add_token() is called from make_and_add_simple_token(), handle_start_state(), add_variable_token(), and handle_in_state().  In each of these functions, add_token() is passed its tokens argument directly from the tokens argument that was passed to them.  Therefore, this argument will always have been checked when add_token() is called, and so add_token() does not need to handle invalid token lists, and I have not tested that it does so.


The following functions rely on command line arguments and reading from files, and so are not testable with quiet assert testing:
- run_lexical_analyser()
- get_tokens_from_file()

The following functions are structure-specific print functions so are not testable with quiet assert testing (also, they are not used in the program, but are relics from debugging that are retained for future debugging):
- print_tokens()
- print_token()

The following functions are used in the lexical_parse_test() function, but are not directly assert tested.  However, they free the memory allocated to the structures made for testing the other functions, and using Valgrind it is seen that running lexical_parse_test() does not result in any memory leaks.  Therefore, we can see that these functions work as intended:
- free_token_list()
- free_token_node()


___parser testing___

I've adopted a 'bottom-up' approach to testing this part of the code.  Because the entire lexer is a recursive call tree - there is no way to test the top funciton (i.e., the function that is called first) without calling all the other functions.  Therefore, my parser_test() function first assert tests the terminal functions (e.g., the functions that handle variables, strings and literals) and then, when we are confident that these work as intended we can take it as a given and test the functions that call them, and so on until we reach to top function.  The full order of this explained below.

Structure of the recursive call tree:

    descend_recursively()
        |
        v
    handle_INSTRCTS() <-
        |               |
        v               |
    handle_INSTRCT() ---
        |
        v
    handle_FUNC()
        |-------------------------------------------------------------------------------------------
        |                                                       |                   |                   |
        v                                                       v                   v                   v
 -> handle_RETFUNC()                                        handle_IOFUNC()*    handle_IF()*        handle_LOOP()
|       |---------------------------------------     -----------|-------------------|                   |
|       |                   |                   |   |                                        ---------------
|       v                   v                   v   v                                       |               |
|   handle_LISTFUNC()   handle_INTFUNC()    handle_BOOLFUNC()                               v               v
|       |                   |                   |                           |-----------handle_SET()    handle_PRINT()#
|       --------------------------------------------------------------------|               |               |
|                           |                                                               |               |
|                           v                                                               v               v
 -----------------------handle_LIST()-------------------------------------------------->handle_VAR()    handle_STRING()
                        |           |
                        v           v
                    handle_NIL()    handle_LITERAL()

*indicates an unshown call to handle_INSTRCTS()
#indicates an unshown call to handle_LIST()

As can be seen from the above, the base functions are handle_NIL(), handle_LITERAL, handle_VAR, and handle_STRING()


___Interpreter testing___

